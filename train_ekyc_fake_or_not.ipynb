{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "- Thu thập data cho 2 tập valid và fake\n",
    "- Link dataset: ```/mnt/ssd/dataset_cccd/```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "- Thử nghiệm với model: resnet50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd/jason/anaconda3/envs/p_light_3_11/lib/python3.11/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/ssd/jason/anaconda3/envs/p_light_3_11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/ssd/jason/anaconda3/envs/p_light_3_11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "import torchmetrics\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from datetime import datetime\n",
    "from torchvision.transforms import v2 as transformsV2\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_NAME = 'resnet50'\n",
    "\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((448, 448)),  \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # transforms.RandomEqualize(),\n",
    "        transforms.ColorJitter(brightness=0, contrast=1.5, saturation=0, hue=0),\n",
    "        transforms.ToTensor(),\n",
    "        # translate, transform, flip, blur\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((448, 448)), \n",
    "        # transforms.RandomVerticalFlip(),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0, contrast=1.5, saturation=0, hue=0),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# data_dir = '/mnt/ssd/dataset_cccd/train_ekyc_moire_or_not/dataset'\n",
    "# full_dataset = ImageFolder(data_dir, transform=data_transforms['train'])\n",
    "\n",
    "\n",
    "# train_size = int(0.8 * len(full_dataset))\n",
    "# val_size = len(full_dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_ds = ImageFolder('/mnt/ssd/ekyc_myanmar/project_fake_profile/dataset_train', transform=data_transforms['train'])\n",
    "\n",
    "# concatenate train_ds_no_padding AND train_ds_with_padding\n",
    "train_data = ConcatDataset([train_ds])\n",
    "\n",
    "\n",
    "val_dataset, train_dataset = random_split(train_data, [0.2, 0.8])\n",
    "train_dataset.dataset.transform = data_transforms['train']\n",
    "val_dataset.dataset.transform = data_transforms['val']\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "class ResNetClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)  # 2 classes: fake, valid\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.confusion_matrix = torchmetrics.ConfusionMatrix(task='multiclass', num_classes=num_classes)\n",
    "        self.wrong_preds_images = {i: [] for i in range(num_classes)}\n",
    "        self.num_classes = num_classes\n",
    "        self.example_input_array = torch.randn(1, 3, 448, 448)\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=0.002, momentum=0.95)\n",
    "        # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, threshold=0.01, patience=8)\n",
    "        return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) \n",
    "        accuracy = self.accuracy(outputs, labels)\n",
    "        self.log(\"train_acc\", accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"outputs_labels\": outputs, \"ground_truth_labels\": labels}\n",
    "\n",
    "\n",
    "    def on_training_epoch_end(self):\n",
    "        pass\n",
    "        \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) \n",
    "        self.accuracy(outputs, labels)\n",
    "        self.log(\"val_acc\", self.accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # self.log('test_loss', loss)\n",
    "        # log step metric\n",
    "        self.accuracy(preds, labels)\n",
    "        self.confusion_matrix(preds, labels)\n",
    "\n",
    "        self.log('test_acc', self.accuracy)\n",
    "\n",
    "         # Collect wrong predictions\n",
    "        wrong_indices = torch.where(preds != labels)[0]\n",
    "        for i in wrong_indices:\n",
    "            true_label = labels[i].item()\n",
    "            self.wrong_preds_images[true_label].append(inputs[i])\n",
    "\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        # Concatenate all wrong predictions\n",
    "        for class_id in range(self.num_classes):\n",
    "            if self.wrong_preds_images[class_id]:\n",
    "                wrong_images = torch.stack(self.wrong_preds_images[class_id])\n",
    "                grid = torchvision.utils.make_grid(wrong_images)\n",
    "                self.logger.experiment.add_image(f'wrong_predictions_class_{class_id}', grid, self.current_epoch)\n",
    "        \n",
    "\n",
    "        val_cm = self.confusion_matrix.compute()\n",
    "        # self.log('test_confusion_matrix', val_cm)\n",
    "        print(f'Validation Confusion Matrix:\\n{val_cm}')\n",
    "        self.confusion_matrix.reset()\n",
    "\n",
    "        return super().on_test_epoch_end()\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=f'checkpoints/resnet50_fake_or_not_ckpt_{current_time}',\n",
    "    filename='{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_acc\", \n",
    "    min_delta=0.00,\n",
    "    patience=15,\n",
    "    verbose=False,\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir='.',\n",
    "    name=f'logs_{MODEL_NAME}'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator='gpu', \n",
    "    precision=16,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    logger=logger  # Add the logger to the Trainer\n",
    ")\n",
    "\n",
    "model = ResNetClassifier()\n",
    "# trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /mnt/ssd/ekyc_myanmar/project_fake_profile/checkpoints/resnet50_fake_or_not_ckpt_20240718_135446/epoch=08-val_loss=0.11-val_acc=0.98.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /mnt/ssd/ekyc_myanmar/project_fake_profile/checkpoints/resnet50_fake_or_not_ckpt_20240718_135446/epoch=08-val_loss=0.11-val_acc=0.98.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 88/88 [00:18<00:00,  4.83it/s]Validation Confusion Matrix:\n",
      "tensor([[1276,  418],\n",
      "        [ 141,  951]], device='cuda:0')\n",
      "Testing DataLoader 0: 100%|██████████| 88/88 [00:38<00:00,  2.26it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.7993538975715637\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.7993538975715637}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import v2 as transformsV2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "import torchmetrics\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "test_transform =  transforms.Compose([\n",
    "        transforms.Resize((448, 448)),\n",
    "        # transforms.ColorJitter(brightness=0, contrast=1.5, saturation=0, hue=0),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.GaussianBlur(kernel_size=(5, 5), sigma=5.),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "\n",
    "    ]) \n",
    "test_dataset_dir = '/mnt/ssd/dataset_cccd/project_fake_profile/dataset_test'\n",
    "test_dataset = ImageFolder(test_dataset_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "model = ResNetClassifier()\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    # max_epochs=100,\n",
    "    accelerator='cuda', \n",
    "    # precision=16,\n",
    "    # callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    # logger=logger  # Add the logger to the Trainer\n",
    ")\n",
    "\n",
    "trainer.test(\n",
    "    model,\n",
    "    ckpt_path='/mnt/ssd/dataset_cccd/project_fake_profile/checkpoints/resnet50_fake_or_not_ckpt_20240718_135446/epoch=08-val_loss=0.11-val_acc=0.98.ckpt',\n",
    "    dataloaders=[test_loader]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip '/mnt/ssd/ekyc_myanmar/train_ekyc_moire_or_not/Device-20240621T030613Z-001.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd/jason/anaconda3/envs/p_light_3_11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/ssd/jason/anaconda3/envs/p_light_3_11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = ResNetClassifier.load_from_checkpoint('/mnt/ssd/dataset_cccd/project_fake_profile/checkpoints/resnet50_fake_or_not_ckpt_20240716_173045_norm_485/epoch=12-val_loss=0.10-val_acc=0.97.ckpt')\n",
    "filepath = \"resnet50_fake_or_not_240718_epoch_12_val_loss_010_val_acc_097.onnx\"\n",
    "model.to_onnx(filepath, export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q '/mnt/ssd/jason/techainer_projects/images/front_good_rotate-20240627T022031Z-001.zip' -d /mnt/ssd/jason/techainer_projects/images/\n",
    "!rm '/mnt/ssd/jason/techainer_projects/images/front_good_rotate-20240627T022031Z-001.zip'\n",
    "\n",
    " # OCR False case > 20242404-0205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip Lib-Card-Validation-Training-dataset.zip -r /mnt/ssd/ekyc_myanmar/Lib-Card-Validation-Training/dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
